# Story 8.3: Replace Raw OpenAI HTTP Call with Agent Runner in `summarize_for_users_in_group`

---

## Status

**Done**

---

## Story

**As a** system operator,
**I want** the summarization pipeline to run LLM calls through the OpenAI Agents SDK runner (not raw `chat.completions.create`),
**so that** every summarization call gets proper Langfuse tracing, SDK-managed retries, and the agent infrastructure is actually used as designed.

---

## Acceptance Criteria

1. `summarize_for_users_in_group` no longer calls `openai_client.chat.completions.create()` directly; all LLM execution goes through the OpenAI Agents SDK `Runner`
2. `summarize_for_users_in_group` accepts a `base_agent: BaseAgent` parameter instead of `openai_client`, `agent_instructions`, and `model`
3. `run_personalized_summarization` passes the `agent` (`BaseAgent`) object directly to `summarize_for_users_in_group` — not its `.instructions` string
4. `run_personalized_summarization` no longer has an `openai_client` parameter; the script-level `OpenAI(api_key=...)` client init in `run_personalized_summarization.py` is removed
5. Token counts in `stats` are extracted from the SDK result's `usage` object (not `response.usage.prompt_tokens`)
6. A Langfuse trace is emitted per post summarization at the group level (no `user_id` — group-level span, operation name `"summarize_group_post"`)
7. All existing 18 tests pass; new unit tests cover: (a) agent runner is called (not `openai_client`), (b) token extraction from SDK result, (c) Langfuse trace created per post

---

## Tasks / Subtasks

- [ ] **Task 1: Refactor `summarize_for_users_in_group` signature and LLM call** (AC: 1, 2, 5)
  - [ ] Change signature from `(... openai_client, agent_instructions: str, model: str)` → `(... base_agent: BaseAgent)`
  - [ ] Add import: `from agents import Runner` at top of `personalized_summarization.py`
  - [ ] Add import: `from app.infrastructure.agents.base_agent import BaseAgent`
  - [ ] Replace `openai_client.chat.completions.create(...)` block with:
    ```python
    result = await Runner.run(base_agent.agent, input=content)
    summary_text = str(result.final_output).strip()
    ```
  - [ ] Extract token counts from `result.usage`:
    ```python
    input_tokens = getattr(result.usage, "input_tokens", 0)
    output_tokens = getattr(result.usage, "output_tokens", 0)
    actual_tokens = input_tokens + output_tokens
    ```
  - [ ] Remove `request_kwargs` dict, `temperature` branch, and the `if not model.startswith("gpt-5")` block (model config lives in `BaseAgent`)
  - [ ] Keep the cost calculation logic but derive it from SDK token fields; simplify cached-token path (SDK does not expose `prompt_tokens_details` the same way — use a simple `input_cost = input_tokens * 0.15 / 1_000_000`, `output_cost = output_tokens * 0.60 / 1_000_000`)

- [ ] **Task 2: Add group-level Langfuse tracing per post** (AC: 6)
  - [ ] At the start of `summarize_for_users_in_group`, initialize Langfuse if configured:
    ```python
    langfuse = None
    if settings.langfuse_enabled and settings.langfuse_public_key:
        from langfuse import Langfuse
        langfuse = Langfuse(
            public_key=settings.langfuse_public_key,
            secret_key=settings.langfuse_secret_key,
            host=settings.langfuse_host,
        )
    ```
  - [ ] Per-post: open a trace with `operation="summarize_group_post"`, `prompt_type` and `post_id` as metadata; end it with output and token usage after the Runner call
  - [ ] Import `AgentSettings` / `settings` from `app.infrastructure.agents.config` inside the function (same lazy-import pattern already used in `run_personalized_summarization`)

- [ ] **Task 3: Update `run_personalized_summarization` to remove `openai_client`** (AC: 3, 4)
  - [ ] Remove `openai_client` parameter from `run_personalized_summarization` signature
  - [ ] Remove `from openai import OpenAI` import and `openai_client=openai_client` kwarg in the call to `summarize_for_users_in_group`
  - [ ] Pass `base_agent=agent` instead of `openai_client=openai_client, agent.instructions, settings.openai_model`

- [ ] **Task 4: Update `scripts/run_personalized_summarization.py`** (AC: 4)
  - [ ] Remove `from openai import OpenAI` import
  - [ ] Remove `openai_client = OpenAI(api_key=settings.openai_api_key)` instantiation in `main_async`
  - [ ] Remove `openai_client=openai_client` from the `run_personalized_summarization(...)` call
  - [ ] Remove the `if not settings.openai_api_key and not args.dry_run:` guard in `main()` — SDK reads `OPENAI_API_KEY` from env directly

- [ ] **Task 5: Update tests** (AC: 7)
  - [ ] In `test_personalized_summarization.py`, replace all `mock_openai_client` fixtures with a `mock_base_agent` fixture that returns a `MagicMock(spec=BaseAgent)` with `mock_base_agent.agent` as the SDK agent mock
  - [ ] Patch `agents.Runner.run` (not `openai_client.chat.completions.create`) in relevant tests
  - [ ] Add `test_summarize_for_users_in_group_uses_runner` — asserts `Runner.run` called once per post (not `openai_client`)
  - [ ] Add `test_summarize_for_users_in_group_extracts_sdk_token_usage` — asserts `stats["total_tokens"]` matches mock `result.usage.input_tokens + result.usage.output_tokens`
  - [ ] Add `test_summarize_for_users_in_group_creates_langfuse_trace` — asserts Langfuse trace created per post when `langfuse_enabled=True`
  - [ ] Run full test suite: `cd backend && uv run pytest tests/ -v` — all 18+ tests must pass

- [ ] **Task 6: Ruff format + lint** (AC: all)
  - [ ] `cd backend && uv run ruff format app/application/use_cases/personalized_summarization.py scripts/run_personalized_summarization.py tests/application/use_cases/test_personalized_summarization.py`
  - [ ] `cd backend && uv run ruff check --fix app/ scripts/ tests/`
  - [ ] Verify no new UP017 violations introduced

---

## Dev Notes

### Root Cause (diagnosed 2026-02-25)

In `run_personalized_summarization` (line 770–807), `create_summarization_agent()` is called which:
1. Builds a `BaseAgent` (which internally instantiates `agents.Agent` from the SDK)
2. Only `agent.instructions` (the combined system prompt string) is extracted and passed downstream

In `summarize_for_users_in_group` (line 619–635), the actual LLM call is:
```python
response = openai_client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": agent_instructions, "cache_control": {"type": "ephemeral"}},
        {"role": "user", "content": content},
    ],
    max_completion_tokens=500,
)
summary_text = response.choices[0].message.content.strip()
```

The SDK `Agent` object, its `Runner`, and `TrackedAgent` are completely bypassed. No Langfuse traces are emitted for summarization. The `summarize_post()` function in `summarization_agent.py` (which correctly uses `TrackedAgent.run()`) is also never called.

### Existing Correct Pattern (reference)

`summarization_agent.py:summarize_post()` (lines 104–156) shows the correct usage:
```python
agent = create_summarization_agent(prompt_type=prompt_type, model=model, ...)
tracked_agent = TrackedAgent(base_agent=agent, user_id=user_id, db_session=db_session, ...)
result = tracked_agent.run(input_text=markdown_content, metadata={...})
```

`TrackedAgent.run()` (in `base_agent.py:119–197`) calls:
```python
result = self.agent.run(input_text)   # agents.Agent.run() — sync SDK call
input_tokens = getattr(result.usage, "input_tokens", 0)
output_tokens = getattr(result.usage, "output_tokens", 0)
output_content = getattr(result, "output", str(result))
```

For Story 8.3, we cannot use `TrackedAgent` directly in the group path because `TrackedAgent` is per-user (requires `user_id`). Instead, use `Runner.run()` (async) directly on `base_agent.agent` and add a group-level Langfuse trace manually.

### Key SDK API (`openai-agents>=0.4.2`)

[Source: tech-stack.md#AI-Agents]

```python
from agents import Runner

# Async execution (use inside async function):
result = await Runner.run(agent, input="article content here")

# Result fields:
summary_text = str(result.final_output).strip()
input_tokens  = getattr(result.usage, "input_tokens", 0)
output_tokens = getattr(result.usage, "output_tokens", 0)
```

`base_agent.agent` is the `agents.Agent` instance (set in `BaseAgent.__init__`, line 49: `self.agent = Agent(...)`).

### Signature Change Summary

**Before:**
```python
async def summarize_for_users_in_group(
    session, content_store, users, posts, prompt_type,
    openai_client,           # ← raw OpenAI client
    agent_instructions: str, # ← instructions string only
    model: str,              # ← model name only
) -> dict:
```

**After:**
```python
async def summarize_for_users_in_group(
    session, content_store, users, posts, prompt_type,
    base_agent: BaseAgent,   # ← full agent object (has .agent, .instructions, .model)
) -> dict:
```

**`run_personalized_summarization` before:**
```python
group_stats = await summarize_for_users_in_group(
    session, content_store, users, posts, prompt_type,
    openai_client,
    agent.instructions,
    settings.openai_model,
)
```

**After:**
```python
group_stats = await summarize_for_users_in_group(
    session, content_store, users, posts, prompt_type,
    base_agent=agent,
)
```

### `run_personalized_summarization.py` Script Change

Remove from `main_async()`:
```python
from openai import OpenAI
openai_client = OpenAI(api_key=settings.openai_api_key)
```
And the `openai_client=openai_client` kwarg in the `run_personalized_summarization(...)` call.

The SDK reads `OPENAI_API_KEY` from the environment directly — no explicit client init needed.

### Langfuse Group Trace Pattern

Follow the pattern already in `TrackedAgent.__init__` and `TrackedAgent.run()` (`base_agent.py:104–197`). For the group path, open a trace without `user_id`:

```python
if langfuse:
    trace = langfuse.trace(
        name="summarize_group_post",
        metadata={"prompt_type": prompt_type, "post_id": str(post_id)},
    )
    generation = trace.generation(
        name="agent_execution",
        model=base_agent.model,
        input=content,
    )
# ... run agent ...
if generation:
    generation.end(
        output=summary_text,
        usage={"input_tokens": input_tokens, "output_tokens": output_tokens},
    )
```

### Cost Calculation Simplification

The existing raw-HTTP cost code reads `response.usage.prompt_tokens_details.cached_tokens`. The SDK result does not expose `prompt_tokens_details` in the same structure. Simplify to:
```python
input_cost = input_tokens * 0.15 / 1_000_000
output_cost = output_tokens * 0.60 / 1_000_000
cost_per_summary = Decimal(str(input_cost + output_cost))
```
The cached-token discount detail is a nice-to-have that can be added back in a future story once the SDK result structure is confirmed.

### Key Files

[Source: source-tree.md#application-use_cases]

| File | Change |
|---|---|
| `backend/app/application/use_cases/personalized_summarization.py` | Replace raw HTTP with `Runner.run()`; update signatures |
| `backend/scripts/run_personalized_summarization.py` | Remove `OpenAI(...)` client init and kwarg |
| `backend/tests/application/use_cases/test_personalized_summarization.py` | Swap `mock_openai_client` → `mock_base_agent`, patch `Runner.run` |

### Testing

[Source: coding-standards.md#Testing-Standards]

- Framework: `pytest` + `pytest-asyncio` (asyncio_mode = "auto")
- Test file: `backend/tests/application/use_cases/test_personalized_summarization.py`
- Run: `cd backend && uv run pytest tests/application/use_cases/test_personalized_summarization.py -v`
- Arrange-Act-Assert pattern; use `pytest.fixture` for shared mocks
- Patch `agents.Runner.run` using `unittest.mock.patch` or `pytest-mock`'s `mocker.patch`
- All 18 existing tests must continue to pass after refactor

### Python 3.10 Compatibility Note

`UP017` (`datetime.UTC`) is suppressed in `pyproject.toml` (added in Story 8.1). Do not use `datetime.UTC` — use `datetime.now(timezone.utc)` instead.

---

## Change Log

| Date | Version | Description | Author |
|---|---|---|---|
| 2026-02-25 | 1.0 | Initial story — raw HTTP bypass diagnosed in `summarize_for_users_in_group` | BMad Master |

---

## Dev Agent Record

### Agent Model Used

_To be filled during implementation_

### Debug Log References

_To be filled during implementation_

### Completion Notes List

_To be filled during implementation_

### File List

_To be filled during implementation_

---

## QA Results

_To be filled by QA Agent_
