# Story 8.2: Add Crawler Daemon Service to docker-compose

---

## Status

**Done**

---

## Story

**As a** system operator,
**I want** article content to be automatically crawled on a schedule as a dedicated Docker service,
**so that** summaries are generated from full article text rather than title/metadata only, improving digest quality.

---

## Acceptance Criteria

1. `crawl_and_store.py` gains a `--daemon` flag that runs it in a loop: crawl → sleep `CRAWLER_INTERVAL_SECONDS` → repeat
2. `docker-compose.yml` has a new `crawler` service using the same `hnpal-app` image, running `crawl_and_store.py --daemon`
3. The crawler service depends on `postgres` and `redis` being healthy (same pattern as `summarizer` and `delivery` services)
4. The crawler only processes posts where `is_crawl_success IS NULL OR is_crawl_success = FALSE` (un-crawled or previously failed), not re-crawling already-successful posts
5. After a crawl run, `is_crawl_success = TRUE` posts have `has_html`, `has_text`, or `has_markdown` flags set in the DB accordingly
6. Crawler interval is configurable via env var `CRAWLER_INTERVAL_SECONDS` (default: `3600` — run once per hour after the posts collector)
7. RocksDB volume is shared with the `summarizer` service (already mounted as `rocksdb_data`) — the crawler writes to it, summarizer reads from it
8. On EC2, `docker ps` shows a running `hnpal-crawler` container alongside existing services

---

## Tasks / Subtasks

- [x] **Task 1: Add `--daemon` mode to `crawl_and_store.py`** (AC: 1, 6)
  - [ ] Add `--daemon` flag to argparse in `scripts/crawl_and_store.py`
  - [ ] Add `--interval` arg (default `3600`) for loop sleep seconds
  - [ ] Wrap `main()` logic in a daemon loop when `--daemon` is set:
    ```python
    while True:
        await run_once(args)
        logger.info(f"Crawler sleeping {args.interval}s until next run...")
        await asyncio.sleep(args.interval)
    ```
  - [ ] Handle `SIGTERM` gracefully (finish current crawl, then exit) so Docker stop works cleanly

- [x] **Task 2: Scope crawl to un-crawled posts only** (AC: 4, 5)
  - [ ] In `save_posts_to_db` (or a new `fetch_uncrawled_posts` query), change the crawl target to posts where `is_crawl_success IS NULL OR (is_crawl_success = FALSE AND crawl_retry_count < 3)`
  - [ ] Skip posts with `is_crawl_success = TRUE` — no re-crawl needed
  - [ ] Log count: `"Found {n} posts to crawl (uncrawled or failed with retries remaining)"`

- [x] **Task 3: Add `crawler` service to `docker-compose.yml`** (AC: 2, 3, 7, 8)
  - [ ] Add new service block following the existing `summarizer` pattern:
    ```yaml
    crawler:
      image: ${WORKER_IMAGE_NAME:-hnpal-app}
      container_name: ${CRAWLER_CONTAINER_NAME:-hnpal-crawler}
      command: ["python", "${CRAWLER_COMMAND_PATH:-scripts/crawl_and_store.py}", "--daemon", "--interval", "${CRAWLER_INTERVAL_SECONDS:-3600}"]
      environment:
        - DATA_DIR=${APP_DATA_DIR:-/app/data}
        - PYTHONPATH=/app
      env_file: .env
      depends_on:
        postgres:
          condition: service_healthy
        redis:
          condition: service_healthy
      volumes:
        - rocksdb_data:${APP_DATA_DIR:-/app/data}
      restart: ${RESTART_POLICY:-unless-stopped}
      mem_limit: ${CRAWLER_MEM_LIMIT:-256m}
    ```
  - [ ] Add env var defaults to `.env.example`: `CRAWLER_CONTAINER_NAME`, `CRAWLER_INTERVAL_SECONDS=3600`, `CRAWLER_MEM_LIMIT=256m`

- [x] **Task 4: Verify RocksDB read/write modes** (AC: 7)
  - [ ] Confirm `crawl_and_store.py` opens RocksDB in **write** mode (not read-only)
  - [ ] Confirm `run_personalized_summarization.py` opens RocksDB in **read-only** mode — check `rocksdb_store.py` for the `read_only` flag
  - [ ] Both services share the same `rocksdb_data` Docker volume — verify no conflict (read-only reader + single writer is safe for RocksDB)

- [ ] **Task 5: Manual verification on remote** (AC: 8)
  - [ ] Deploy updated `docker-compose.yml` to EC2 (`docker compose up -d`)
  - [ ] Verify `docker ps` shows `hnpal-crawler` running
  - [ ] After first crawler run, check DB: `SELECT COUNT(*) FROM posts WHERE is_crawl_success = TRUE` — should be > 0
  - [ ] Check RocksDB content: run `docker exec hnpal-crawler python -c "from app.infrastructure.storage.rocksdb_store import RocksDBContentStore; ..."` to confirm entries exist

---

## Dev Notes

### Root Cause (from 2026-02-25 diagnosis)

All 115 collected posts showed `is_crawl_success = FALSE` with empty `crawl_error`. The `crawl_and_store.py` script exists and is fully implemented but was **never added to `docker-compose.yml`** as a service. Posts were summarized using title/metadata fallback only (`get_post_content` falls back to constructing markdown from `title`, `url`, `score`, `comment_count`).

### Current docker-compose Services (before this story)

| Container | Script | Role |
|---|---|---|
| `hnpal-trigger-posts` | `trigger_posts_collection.py --daemon` | Hourly HN metadata collector |
| `hnpal-summarizer` | `run_personalized_summarization.py --daemon` | Summarizes new posts every 30min |
| `hnpal-delivery` | `run_delivery_daemon.py --daemon` | Sends digests to Telegram every hour |
| `hnpal-bot` | `run_bot.py` | Telegram bot command handler |
| `hnpal-redis` | redis:7-alpine | Session/state store |
| `hnpal-postgres` | postgres:15-alpine | Primary DB |
| **`hnpal-crawler`** | **`crawl_and_store.py --daemon`** | **MISSING — this story adds it** |

### Timing Coordination

Recommended run order per hour:
1. `:00` — `trigger-posts` collects new HN posts (metadata)
2. `:05` — `crawler` runs (crawls article content for new posts)
3. `:30` — `summarizer` runs (now has content available in RocksDB)
4. `:00+1h` — `delivery` sends digest

The crawler interval of 3600s (1h) fits this naturally. No explicit coordination needed — each service is idempotent and the summarizer falls back to metadata if content isn't ready yet.

### RocksDB Concurrency

RocksDB supports a **single writer + multiple read-only readers** safely. The crawler is the only writer; the summarizer opens it read-only (confirmed in `rocksdb_store.py` — `RocksDBContentStore` has a `read_only` parameter). Both share the `rocksdb_data` named Docker volume.

### Crawl Retry Logic

`crawl_retry_count` is incremented on each failed crawl (see `post_repo.py:update_crawl_status`). Cap retries at 3 to avoid hammering unreachable URLs. Posts with `crawl_retry_count >= 3` should be left as failed permanently.

### Key Files

| File | Change |
|---|---|
| `backend/scripts/crawl_and_store.py` | Add `--daemon` flag + uncrawled-only filter |
| `docker-compose.yml` | Add `crawler` service block |
| `.env.example` | Add `CRAWLER_*` env var defaults |

### Testing

- Run crawler once manually before adding daemon: `docker exec hnpal-summarizer python scripts/crawl_and_store.py --limit 10`
- Verify DB update: `SELECT hn_id, is_crawl_success, has_html, has_text FROM posts WHERE is_crawl_success = TRUE LIMIT 5`
- Verify RocksDB write: check stats via `crawler.content_store.get_stats()`

---

## Change Log

| Date | Version | Description | Author |
|---|---|---|---|
| 2026-02-25 | 1.0 | Initial story — crawler service missing from docker-compose, diagnosed on remote EC2 | BMad Master |

---

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-6

### Debug Log References

None — straightforward implementation, no debugging needed.

### Completion Notes List

- Task 4 (RocksDB modes): Verified `RocksDBContentStore` defaults to `read_only=False` (write mode). The `summarizer` service passes `read_only=True`. Crawler uses the default write mode. Safe single-writer + read-only-readers pattern confirmed.
- Task 5 (manual EC2 verification) left for operator — requires live deployment.
- The `run_once` function fetches top HN posts, saves new ones to DB, then queries all uncrawled posts from DB to crawl. This ensures previously failed posts (retry_count < 3) also get retried each cycle.

### File List

- `backend/scripts/crawl_and_store.py`
- `backend/app/infrastructure/repositories/postgres/post_repo.py`
- `docker-compose.yml`
- `backend/.env.example`

---

## QA Results

_To be filled by QA Agent_
