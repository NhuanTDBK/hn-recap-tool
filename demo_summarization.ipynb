{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d688695",
   "metadata": {},
   "source": [
    "# OpenAI Summarization Service Demo\n",
    "\n",
    "This notebook demonstrates how to use the `OpenAISummarizationService` with fake input data to test summarization capabilities including:\n",
    "- Single content summarization\n",
    "- Long content with map-reduce strategy\n",
    "- Batch summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc645c9d",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bd097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "from backend.app.infrastructure.services.openai_summarization_service import OpenAISummarizationService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17b206",
   "metadata": {},
   "source": [
    "## Initialize the Service\n",
    "\n",
    "Create an instance of the summarization service. Make sure you have your OpenAI API key set in the environment or settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab36dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Summarization service initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the service\n",
    "# Note: API key should be set in your environment variables or settings\n",
    "service = OpenAISummarizationService(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.4,\n",
    "    chunk_size=8000\n",
    ")\n",
    "\n",
    "print(\"✓ Summarization service initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254be14",
   "metadata": {},
   "source": [
    "## Test 1: Short Content Summarization\n",
    "\n",
    "Summarize a short article about artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7516d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 7612 characters\n",
      "\n",
      "Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "DeepSeek-OCR introduces \"contexts optical compression,\" converting text into images to reduce token usage by up to 20x, maintaining 97% accuracy at 10x compression and 60% at 20x. This method enhances efficiency in processing extensive documents, surpassing traditional OCR models. ([deepseek-ocr.ai](https://www.deepseek-ocr.ai/?utm_source=openai))\n",
      "\n",
      "The model comprises DeepEncoder, which compresses high-resolution text into images, and DeepSeek3B-MoE-A570M, a decoder that reconstructs text from these images. This approach is particularly effective for complex documents, including those with tables and charts. ([deepseekocr.io](https://deepseekocr.io/?utm_source=openai))\n",
      "\n",
      "While the concept of treating text as images is innovative, it may not be universally applicable. The effectiveness of this method depends on the specific requirements of the task and the nature of the documents involved.\n",
      "\n",
      "In summary, DeepSeek-OCR's optical compression offers a promising avenue for efficient document processing, especially for complex layouts, though its applicability may vary based on specific use cases.\n",
      "\n",
      "#DeepSeekOCR #OpticalCompression #DocumentProcessing #AIInnovation \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fake short content\n",
    "short_content = \"\"\"\n",
    "Should LLMs just treat text content as an image?\n",
    "Several days ago, DeepSeek released a new OCR paper. OCR, or “optical character recognition”, is the process of converting an image of text - say, a scanned page of a book - into actual text content. Better OCR is obviously relevant to AI because it unlocks more text data to train language models on1. But there’s a more subtle reason why really good OCR might have deep implications for AI models.\n",
    "Optical compression\n",
    "According to the DeepSeek paper, you can pull out 10 text tokens from a single image token with near-100% accuracy. In other words, a model’s internal representation of an image is ten times as efficient as its internal representation of text. Does this mean that models shouldn’t consume text at all? When I paste a few paragraphs into ChatGPT, would it be more efficient to convert that into an image of text before sending it to the model? Can we supply 10x or 20x more data to a model at inference time by supplying it as an image of text instead of text itself?\n",
    "This is called “optical compression”. It reminds me of a funny idea from June of this year to save money on OpenAI transcriptions: before uploading the audio, run it through ffmpeg to speed it up by 2x. The model is smart enough to still pull out the text, and with one simple trick you’ve cut your inference costs and time by half. Optical compression is the same kind of idea: before uploading a big block of text, take a screenshot of it (and optionally downscale the quality) and upload the screenshot instead.\n",
    "Some people are already sort-of doing this with existing multimodal LLMs. There’s a company selling this as a service, an open-source project, and even a benchmark. It seems to work okay! Bear in mind that this is not an intended use case for existing models, so it’s plausible that it could get a lot better if AI labs start actually focusing on it.\n",
    "The DeepSeek paper suggests an interesting way2 to use tighter optical compression for long-form text contexts. As the context grows, you could decrease the resolution of the oldest images so they’re cheaper to store, but are also literally blurrier. The paper suggests an analogy between this and human memory, where fresh memories are quite vivid but older ones are vaguer and have less detail.\n",
    "Why would this work?\n",
    "Optical compression is pretty unintuitive to many software engineers. Why on earth would an image of text be expressible in fewer tokens than the text itself?\n",
    "In terms of raw information density, an image obviously contains more information than its equivalent text. You can test this for yourself by creating a text file, screenshotting the page, and comparing the size of the image with the size of the text file: the image is about 200x larger. Intuitively, the word “dog” only contains a single word’s worth of information, while an image of the word “dog” contains information about the font, the background and text color, kerning, margins, and so on. How, then, could it be possible that a single image token can contain ten tokens worth of text?\n",
    "The first explanation is that text tokens are discrete while image tokens are continuous. Each model has a finite number of text tokens - say, around 50,000. Each of those tokens corresponds to an embedding of, say, 1000 floating-point numbers. Text tokens thus only occupy a scattering of single points in the space of all possible embeddings. By contrast, the embedding of an image token can be any sequence of those 1000 numbers. So an image token can be far more expressive than a series of text tokens.\n",
    "Another way of looking at the same intuition is that text tokens are a really inefficient way of expressing information. This is often obscured by the fact that text tokens are a reasonably efficient way of sharing information, so long as the sender and receiver both know the list of all possible tokens. When you send a LLM a stream of tokens and it outputs the next one, you’re not passing around slices of a thousand numbers for each token - you’re passing a single integer that represents the token ID. But inside the model this is expanded into a much more inefficient representation (inefficient because it encodes some amount of information about the meaning and use of the token)3. So it’s not that surprising that you could do better than text tokens.\n",
    "Zooming out a bit, it’s plausible to me that processing text as images is closer to how the human brain works. To state the obvious, humans don’t consume text as textual content; we consume it as image content (or sometimes as audio). Maybe treating text as a sub-category of image content could unlock ways of processing text that are unavailable when you’re just consuming text content. As a toy example, emoji like :) are easily-understandable as image content but require you to “already know the trick” as text content4.\n",
    "Final thoughts\n",
    "Of course, AI research is full of ideas that sounds promising but just don’t work that well. It sounds like you should be able to do this trick on current multimodal LLMs - particularly since many people just use them for OCR purposes anyway - but it hasn’t worked well enough to become common practice.\n",
    "Could you train a new large language model on text represented as image content? It might be tricky. Training on text tokens is easy - you can simply take a string of text and ask the model to predict the next token. How do you train on an image of text?\n",
    "You could break up the image into word chunks and ask the model to generate an image of the next word. But that seems to me like it’d be really slow, and tricky to check if the model was correct or not (e.g. how do you quickly break a file into per-word chunks, how do you match the next word in the image, etc). Alternatively, you could ask the model to output the next word as a token. But then you probably have to train the model on enough tokens so it knows how to manipulate text tokens. At some point you’re just training a normal LLM with no special “text as image” superpowers.\n",
    "edit: this post got some comments on Hacker News. Some commenters are suspicious that image tokenization could ever be better than text tokenization, while other commenters say they’ve already been supplying text-as-image prompts successfully.\n",
    "edit: I also remembered a relevant point from my past amateur research into owl call identification. State of the art bird call identifier systems like BirdNet do so by visual processing, not audio processing - they convert the audio into a spectrogram and then run that image through a CNN, instead of directly embedding the audio stream.\n",
    "-\n",
    "AI labs are desperate for high-quality text, but only around 30% of written books have been digitized. It’s really hard to find recent data on this, but as a very rough estimate Google Books had ~40M books in 2023, but Google estimates there to have been ~130M books in 2010. That comes out to 30%.\n",
    "↩ -\n",
    "See Figure 13.\n",
    "↩ -\n",
    "Not to skip too far ahead, but this is one reason to think that representing a block of text tokens in a single image might not be such a great idea.\n",
    "↩ -\n",
    "Of course current LLMs can interpret these emojis. Less-toy examples: image-based LLMs might have a better feel for paragraph breaks and headings, might be better able to take a big picture view of a single page of text, and might find it easier to “skip through” large documents by skimming the start of each paragraph. Or they might not! We won’t know until somebody tries.\n",
    "↩\n",
    "If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.\n",
    "October 21, 2025 │ Tags: ai\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the content\n",
    "summary = await service.summarize(short_content)\n",
    "\n",
    "print(\"Original length:\", len(short_content), \"characters\")\n",
    "print(\"\\nSummary:\")\n",
    "print(\"-\" * 80)\n",
    "print(summary)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29ca8",
   "metadata": {},
   "source": [
    "## Test 2: Long Content with Map-Reduce\n",
    "\n",
    "Test the map-reduce strategy with a longer article that will be split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22573e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake long content (simulating a long article)\n",
    "long_content = \"\"\"\n",
    "The Evolution of Web Development: A Comprehensive Overview\n",
    "\n",
    "Introduction:\n",
    "Web development has undergone tremendous changes since the early days of the internet. \n",
    "What started as simple static HTML pages has evolved into complex, interactive \n",
    "applications that power modern digital experiences. This article explores the major \n",
    "milestones in web development history and emerging trends shaping its future.\n",
    "\n",
    "The Early Days (1990s):\n",
    "In the beginning, websites were primarily static HTML documents with basic styling \n",
    "provided by CSS. Developers had limited tools and browsers had inconsistent support \n",
    "for web standards. JavaScript was introduced in 1995, enabling basic interactivity, \n",
    "but it was primitive compared to today's capabilities. Web design was constrained \n",
    "by slow internet connections and limited browser capabilities.\n",
    "\n",
    "The Rise of Dynamic Web (2000s):\n",
    "The 2000s saw the emergence of server-side technologies like PHP, ASP, and JSP, \n",
    "enabling dynamic content generation. AJAX revolutionized user experience by allowing \n",
    "partial page updates without full reloads. Content Management Systems like WordPress \n",
    "and Drupal made it easier for non-technical users to create and manage websites. \n",
    "Web 2.0 brought user-generated content and social networking to the forefront.\n",
    "\n",
    "Modern Web Development (2010s):\n",
    "The 2010s introduced powerful JavaScript frameworks like Angular, React, and Vue.js, \n",
    "enabling the creation of Single Page Applications (SPAs). Mobile-first design became \n",
    "essential as smartphone usage exploded. Responsive web design techniques ensured \n",
    "websites worked seamlessly across different screen sizes. RESTful APIs and later \n",
    "GraphQL enabled better separation between frontend and backend systems.\n",
    "\n",
    "Cloud Computing and DevOps:\n",
    "Cloud platforms like AWS, Azure, and Google Cloud transformed how applications are \n",
    "deployed and scaled. Containerization with Docker and orchestration with Kubernetes \n",
    "became standard practices. Continuous Integration and Continuous Deployment (CI/CD) \n",
    "pipelines automated the development workflow. Serverless architectures emerged, \n",
    "allowing developers to focus on code rather than infrastructure management.\n",
    "\n",
    "Current Trends and Future Directions:\n",
    "Progressive Web Apps (PWAs) blur the line between web and native applications, \n",
    "offering offline functionality and app-like experiences. WebAssembly enables \n",
    "near-native performance for compute-intensive tasks in the browser. JAMstack \n",
    "architecture promotes better performance, security, and scalability through \n",
    "pre-rendering and CDN distribution. Artificial Intelligence and Machine Learning \n",
    "are being integrated into web applications for personalization and automation.\n",
    "\n",
    "Performance Optimization:\n",
    "Modern web development emphasizes performance optimization through techniques like \n",
    "code splitting, lazy loading, and image optimization. Core Web Vitals have become \n",
    "important metrics for measuring user experience. Edge computing brings computation \n",
    "closer to users, reducing latency. Build tools like Webpack, Vite, and esbuild \n",
    "optimize bundle sizes and improve load times.\n",
    "\n",
    "Security Considerations:\n",
    "Web security has become increasingly critical with the rise of cyber threats. \n",
    "HTTPS is now standard, with browsers warning users about insecure connections. \n",
    "Content Security Policy (CSP) helps prevent XSS attacks. OAuth and JWT provide \n",
    "secure authentication mechanisms. Regular security audits and dependency updates \n",
    "are essential practices for maintaining secure applications.\n",
    "\n",
    "The Future of Web Development:\n",
    "Looking ahead, WebGPU promises to bring high-performance graphics to the browser. \n",
    "Web3 technologies are exploring decentralized applications and blockchain integration. \n",
    "Voice interfaces and AR/VR experiences are becoming more prevalent on the web. \n",
    "The focus on accessibility ensures websites are usable by everyone, regardless of \n",
    "abilities. Low-code and no-code platforms are democratizing web development.\n",
    "\n",
    "Conclusion:\n",
    "Web development continues to evolve at a rapid pace, driven by technological \n",
    "advancements and changing user expectations. Developers must stay current with \n",
    "new tools, frameworks, and best practices to build effective web applications. \n",
    "The future promises even more exciting innovations that will further transform \n",
    "how we create and interact with web content.\n",
    "\"\"\" * 3  # Repeat to make it longer\n",
    "\n",
    "# Summarize the long content\n",
    "print(f\"Long content length: {len(long_content)} characters\")\n",
    "print(f\"Chunk size: {service.chunk_size} characters\")\n",
    "print(\"\\nSummarizing (this may take a moment for map-reduce)...\")\n",
    "\n",
    "long_summary = await service.summarize(long_content)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"-\" * 80)\n",
    "print(long_summary)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5856a",
   "metadata": {},
   "source": [
    "## Test 3: Batch Summarization\n",
    "\n",
    "Summarize multiple articles concurrently using batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e017ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake batch content - multiple articles\n",
    "batch_contents = [\n",
    "    \"\"\"\n",
    "    Quantum computing represents a paradigm shift in computational power. Unlike \n",
    "    classical computers that use bits (0s and 1s), quantum computers use qubits \n",
    "    that can exist in superposition. This allows them to perform certain calculations \n",
    "    exponentially faster than classical computers. Companies like IBM, Google, and \n",
    "    Microsoft are racing to build practical quantum computers. Applications include \n",
    "    cryptography, drug discovery, and optimization problems.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Climate change is one of the most pressing challenges facing humanity. Rising \n",
    "    global temperatures are causing more frequent extreme weather events, melting \n",
    "    ice caps, and rising sea levels. Scientists agree that human activities, \n",
    "    particularly greenhouse gas emissions, are the primary cause. Solutions include \n",
    "    transitioning to renewable energy, improving energy efficiency, and developing \n",
    "    carbon capture technologies. International cooperation is essential to address \n",
    "    this global challenge.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    The gig economy has transformed how people work, offering flexibility and \n",
    "    independence. Platforms like Uber, Airbnb, and Upwork connect workers with \n",
    "    customers directly. While this provides opportunities for supplemental income \n",
    "    and entrepreneurship, it also raises concerns about job security, benefits, \n",
    "    and worker protections. Policymakers are grappling with how to regulate these \n",
    "    new forms of employment while preserving their benefits.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Space exploration has entered a new era with private companies joining government \n",
    "    agencies. SpaceX has successfully launched reusable rockets, dramatically reducing \n",
    "    launch costs. NASA's Artemis program aims to return humans to the Moon and \n",
    "    eventually reach Mars. Commercial space stations and space tourism are becoming \n",
    "    reality. The next frontier includes asteroid mining and establishing permanent \n",
    "    human presence beyond Earth.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Batch summarize\n",
    "print(f\"Summarizing {len(batch_contents)} articles concurrently...\\n\")\n",
    "\n",
    "batch_summaries = await service.summarize_batch(batch_contents)\n",
    "\n",
    "# Display results\n",
    "for i, (original, summary) in enumerate(zip(batch_contents, batch_summaries), 1):\n",
    "    print(f\"Article {i}:\")\n",
    "    print(f\"Original length: {len(original)} characters\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dd03d",
   "metadata": {},
   "source": [
    "## Test 4: Error Handling\n",
    "\n",
    "Test how the service handles edge cases and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00415f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test empty content\n",
    "print(\"Testing empty content handling...\")\n",
    "try:\n",
    "    await service.summarize(\"\")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Correctly caught error: {e}\")\n",
    "\n",
    "# Test whitespace-only content\n",
    "print(\"\\nTesting whitespace-only content...\")\n",
    "try:\n",
    "    await service.summarize(\"   \\n\\t   \")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Correctly caught error: {e}\")\n",
    "\n",
    "print(\"\\n✓ Error handling works as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a402ab",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✓ Basic single content summarization\n",
    "2. ✓ Long content summarization with automatic chunking (map-reduce)\n",
    "3. ✓ Batch summarization of multiple items concurrently\n",
    "4. ✓ Error handling for invalid inputs\n",
    "\n",
    "The `OpenAISummarizationService` successfully handles various content lengths and uses map-reduce strategy automatically when content exceeds the chunk size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackernews_digest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
