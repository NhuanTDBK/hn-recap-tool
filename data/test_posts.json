[
  {
    "hn_id": 1,
    "type": "story",
    "title": "Building a High-Performance Key-Value Store with RocksDB",
    "url": "https://example.com/rocksdb-kv-store",
    "author": "alex_tech",
    "score": 450,
    "comment_count": 89,
    "markdown_content": "# Building a High-Performance Key-Value Store with RocksDB\n\nRocksDB is a high-performance, embeddable key-value store library developed by Facebook. In this article, we explore how to build a production-grade key-value store using RocksDB as the foundation.\n\n## Architecture Overview\n\nRocksDB uses an LSM tree (Log-Structured Merge tree) architecture, which is optimized for write-heavy workloads. Unlike B-trees which are optimized for reads, LSM trees batch writes and merge them efficiently.\n\nKey architectural decisions:\n1. **Column Families**: Organize data into logical groups\n2. **Compression**: Built-in Zstandard compression reduces storage by ~70%\n3. **Compaction**: Background process merges SST files\n4. **Bloom Filters**: O(1) key lookups across LSM levels\n\n## Performance Characteristics\n\nBenchmarks on commodity hardware (Intel Xeon, 32GB RAM):\n- **Write throughput**: 500,000 writes/second\n- **Read throughput**: 300,000 reads/second\n- **Point lookup**: p99 latency 50-100 microseconds\n- **Range scans**: ~1 million keys/second\n\n## Implementation Details\n\nWe implemented a wrapper library that provides:\n- Async/await support for concurrent operations\n- Connection pooling for multi-threaded access\n- Automatic backup and recovery\n- Metrics collection and monitoring\n\n## Trade-offs\n\n**Advantages**: Write-optimized, embeddable, no separate daemon process\n**Disadvantages**: Not suitable for distributed systems (single-node), limited query flexibility\n\n## Conclusion\n\nRocksDB is excellent for single-node, write-heavy workloads like time-series databases, caches, and logging systems. The LSM tree architecture provides predictable performance and efficient storage utilization."
  },
  {
    "hn_id": 2,
    "type": "story",
    "title": "How Stripe Reduced Payment Failures by 40% with Event Sourcing",
    "url": "https://example.com/stripe-event-sourcing",
    "author": "payment_eng",
    "score": 520,
    "comment_count": 156,
    "markdown_content": "# How Stripe Reduced Payment Failures by 40% with Event Sourcing\n\nPayment processing is mission-critical infrastructure. At Stripe, we handle billions of dollars in transactions annually. Recently, we implemented event sourcing to dramatically improve reliability and debugging capabilities.\n\n## The Problem\n\nOur legacy payment orchestration system used a traditional CRUD model:\n- Customer initiates payment\n- Payment state is updated in database\n- External services (banks, card networks) are called\n- State is updated again\n\nIssues we faced:\n1. State inconsistencies when external calls failed\n2. Impossible to replay or debug payment flows\n3. Race conditions in concurrent payment updates\n4. Difficult to implement compensating transactions\n\n## Event Sourcing Solution\n\nInstead of storing final state, we store every event:\n- PaymentInitiated\n- AuthorizationAttempted\n- AuthorizationSucceeded / AuthorizationFailed\n- ChargeProcessed / ChargeFailed\n- Settlement Completed\n\nBenefits:\n1. **Complete audit trail**: Every state change is immutable\n2. **Replay capability**: Reconstruct any payment state at any time\n3. **Compensating transactions**: Safely reverse failed payments\n4. **Debugging**: View exact sequence of events\n\n## Architecture\n\nWe use Kafka for event streaming:\n- EventStore: Append-only Kafka topics\n- Event Handlers: Subscribe and process events\n- Event Projections: Build read models from events\n- Snapshot Store: Cache current state for performance\n\n## Results\n\n**Payment failure rate improved from 2.3% to 1.4%** (40% reduction)\n- Faster recovery from partial failures\n- Reduced manual intervention by 80%\n- Debugging time reduced from hours to minutes\n\n## Lessons Learned\n\n1. **Consistency**: Events are the source of truth, not projections\n2. **Idempotency**: Event handlers must be idempotent (handle duplicates)\n3. **Versioning**: Plan for event schema evolution\n4. **Snapshots**: Use snapshots to avoid replaying millions of events\n\n## Conclusion\n\nEvent sourcing transformed our payment system from fragile to resilient. The complete event history enables debugging, recovery, and compliance auditing. The key insight: domain events as the source of truth provide better observability than CRUD operations."
  },
  {
    "hn_id": 3,
    "type": "ask",
    "title": "Ask HN: What's the best way to learn systems design?",
    "url": null,
    "author": "curious_dev",
    "score": 310,
    "comment_count": 203,
    "markdown_content": "# Ask HN: What's the best way to learn systems design?\n\nI'm a backend engineer with 3 years of experience. I want to level up my systems design skills for senior engineering roles. What resources, projects, or approaches have helped you?\n\n## Context\n\nI currently:\n- Build REST APIs in Python\n- Understand databases and caching basics\n- Have deployed services to production\n- Know about load balancing and monitoring\n\nI want to:\n- Design scalable systems from scratch\n- Make better architectural decisions\n- Understand trade-offs (CAP theorem, etc.)\n- Be ready for systems design interviews\n\n## Questions\n\n1. What are the essential concepts I should master?\n2. What real-world systems should I study?\n3. Are there good courses or books?\n4. Should I build projects or study theory first?\n5. How long typically to get proficient?\n\nAppreciate any guidance!"
  },
  {
    "hn_id": 4,
    "type": "story",
    "title": "SQLite's New JSONB Implementation Outperforms PostgreSQL",
    "url": "https://example.com/sqlite-jsonb-perf",
    "author": "db_researcher",
    "score": 680,
    "comment_count": 412,
    "markdown_content": "# SQLite's New JSONB Implementation Outperforms PostgreSQL\n\nSQLite 3.45 introduces a new JSONB (JSON Binary) format that dramatically improves JSON storage and query performance. In our benchmarks, SQLite JSONB outperforms PostgreSQL's JSONB implementation by 3-5x on common operations.\n\n## Background\n\nJSON has become ubiquitous in modern applications. Both SQLite and PostgreSQL support JSON storage:\n- PostgreSQL JSONB: Binary format, indexed, queryable\n- SQLite JSON: Text format, limited indexing\n\nSQLite's new JSONB closes the gap with a more efficient binary representation.\n\n## JSONB Format\n\nKey improvements:\n1. **Binary encoding**: Efficient variable-length encoding\n2. **Compact representation**: ~30% smaller than JSON text\n3. **Built-in indexes**: Faster path lookups\n4. **Schema validation**: Optional JSON schema enforcement\n\n## Performance Benchmarks\n\nTesting on 1M documents with nested structures:\n\n| Operation | PostgreSQL | SQLite JSONB | Speedup |\n|-----------|-----------|------------|----------|\n| Point lookup | 2.5ms | 0.8ms | 3.1x |\n| Range query | 15ms | 3.2ms | 4.7x |\n| Aggregation | 50ms | 12ms | 4.2x |\n| Update single field | 8ms | 1.5ms | 5.3x |\n\n## Storage Comparison\n\n- PostgreSQL JSONB: 125GB\n- SQLite JSONB: 95GB (24% reduction)\n- SQLite JSON: 340GB\n\n## Query Language\n\nSQLite JSONB supports a SQL extension for queries:\n```sql\nSELECT json->>'name' as name\nFROM users\nWHERE json->'age' > 30\n```\n\n## Implications\n\n1. **Embedded systems**: SQLite becomes competitive with PostgreSQL for embedded use cases\n2. **Data locality**: Smaller JSONB allows more data in CPU cache\n3. **Mobile apps**: Reduced storage footprint on mobile devices\n4. **Competitive pressure**: PostgreSQL may need to optimize further\n\n## Caveats\n\n1. SQLite remains single-threaded (not distributed)\n2. PostgreSQL's ecosystem (extensions, replication) is more mature\n3. JSONB format not standardized (SQLite-specific)\n\n## Conclusion\n\nSQLite's new JSONB is a significant step forward. For single-node, embedded, or mobile applications, SQLite is now the better choice for JSON workloads. PostgreSQL remains superior for distributed systems and complex queries."
  },
  {
    "hn_id": 5,
    "type": "show",
    "title": "Show HN: Distributed Rate Limiter Built with Redis",
    "url": "https://github.com/example/distributed-rate-limiter",
    "author": "open_source_dev",
    "score": 420,
    "comment_count": 127,
    "markdown_content": "# Show HN: Distributed Rate Limiter Built with Redis\n\nI built a distributed rate limiter using Redis that works across multiple servers. It supports multiple rate-limiting algorithms and is production-ready.\n\n## Features\n\n- **Multiple algorithms**: Token bucket, sliding window, fixed window\n- **Distributed**: Works across multiple servers via Redis\n- **Atomic operations**: Lua scripts ensure consistency\n- **Per-user limits**: Track limits per user, IP, or custom key\n- **Flexible configuration**: Set limits per second, minute, hour, day\n- **Low latency**: Single Redis round-trip per request (~1-5ms)\n\n## Architecture\n\n```\nAPI Requests → Middleware → Redis Check → Allow/Deny\n                    ↓\n              Lua Script (atomic)\n              ↓\n         Increment counter\n         Check threshold\n         Return result\n```\n\n## Algorithms\n\n### Token Bucket\n- Allows burst traffic up to bucket capacity\n- Refills at configurable rate\n- Good for API rate limiting\n\n### Sliding Window\n- Counts requests in rolling time window\n- Most accurate, slightly more expensive\n- Best for strict limits\n\n### Fixed Window\n- Simplest, most efficient\n- Resets at fixed intervals\n- Can allow bursts at window boundaries\n\n## Performance\n\nBenchmarks (Intel i7, local Redis):\n- **Throughput**: 50,000 checks/second per Redis connection\n- **Latency**: p50: 0.5ms, p99: 2ms, p999: 5ms\n- **Memory**: ~1KB per unique key\n\n## Usage Example\n\n```python\nfrom rate_limiter import RateLimiter\n\nlimiter = RateLimiter(redis_client, algorithm='token_bucket')\n\nif limiter.is_allowed(user_id, limit=100, window=3600):\n    # Allow request\n    process_request()\nelse:\n    # Reject request\n    return 429\n```\n\n## Implementation Details\n\n- Written in Python with no external dependencies\n- Uses Redis Lua scripts for atomic operations\n- Supports Redis cluster and sentinel\n- Comprehensive logging and metrics\n\n## Open Source\n\nAvailable on GitHub: https://github.com/example/distributed-rate-limiter\nMIT License, PRs welcome!\n\n## Future Work\n\n- Prometheus metrics export\n- Rate limit preview endpoint\n- Graphite/StatsD integration"
  }
]
